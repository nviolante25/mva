{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1agMjx1r34og"
      },
      "source": [
        "# Algorithms for Speech and NLP\n",
        "MVA 2021-2022\n",
        "\n",
        "David Soto: david.soto.c17@gmail.com\n",
        "\n",
        "Elias Masquil: eliasmasquil@gmail.com\n",
        "\n",
        "Nicolas Violante: nviolante96@gmail.com"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "In this project we compare the performance on classification downstream tasks of the character-based language model CANINE against the subword-based mBERT. For both cases, we use the pre-trained models available at the Hugging Face Hub  and fine-tune them for the particular downstream task. "
      ],
      "metadata": {
        "id": "M6blB5X_se7r"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "*References*:\n",
        "\n",
        "\n",
        "*   [CANINE: Pre-training an Efficient Tokenization-Free Encoder for Language Representation](https://arxiv.org/abs/2103.06874v3)\n",
        "* [BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding](https://arxiv.org/abs/1810.04805)\n",
        "*   [Hugging Face tutorials](https://huggingface.co/docs/transformers/training#trainer)\n",
        "\n"
      ],
      "metadata": {
        "id": "QgiAUySpt3_C"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Installs, imports and Colab-Drive settings"
      ],
      "metadata": {
        "id": "cGOFiBSxte1h"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2XEkydKa8KxT",
        "outputId": "c444b21d-936f-43a0-dc4b-1cbd9d948f68",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting transformers\n",
            "  Downloading transformers-4.18.0-py3-none-any.whl (4.0 MB)\n",
            "\u001b[K     |████████████████████████████████| 4.0 MB 19.6 MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (1.21.6)\n",
            "Collecting huggingface-hub<1.0,>=0.1.0\n",
            "  Downloading huggingface_hub-0.5.1-py3-none-any.whl (77 kB)\n",
            "\u001b[K     |████████████████████████████████| 77 kB 5.4 MB/s \n",
            "\u001b[?25hRequirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers) (4.64.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (2019.12.20)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from transformers) (4.11.3)\n",
            "Collecting tokenizers!=0.11.3,<0.13,>=0.11.1\n",
            "  Downloading tokenizers-0.12.1-cp37-cp37m-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (6.6 MB)\n",
            "\u001b[K     |████████████████████████████████| 6.6 MB 49.2 MB/s \n",
            "\u001b[?25hRequirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers) (2.23.0)\n",
            "Collecting sacremoses\n",
            "  Downloading sacremoses-0.0.49-py3-none-any.whl (895 kB)\n",
            "\u001b[K     |████████████████████████████████| 895 kB 33.8 MB/s \n",
            "\u001b[?25hRequirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from transformers) (21.3)\n",
            "Collecting pyyaml>=5.1\n",
            "  Downloading PyYAML-6.0-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (596 kB)\n",
            "\u001b[K     |████████████████████████████████| 596 kB 33.0 MB/s \n",
            "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers) (3.6.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.7/dist-packages (from huggingface-hub<1.0,>=0.1.0->transformers) (4.1.1)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->transformers) (3.0.8)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->transformers) (3.8.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2021.10.8)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2.10)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (7.1.2)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.1.0)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.15.0)\n",
            "Installing collected packages: pyyaml, tokenizers, sacremoses, huggingface-hub, transformers\n",
            "  Attempting uninstall: pyyaml\n",
            "    Found existing installation: PyYAML 3.13\n",
            "    Uninstalling PyYAML-3.13:\n",
            "      Successfully uninstalled PyYAML-3.13\n",
            "Successfully installed huggingface-hub-0.5.1 pyyaml-6.0 sacremoses-0.0.49 tokenizers-0.12.1 transformers-4.18.0\n",
            "Collecting datasets\n",
            "  Downloading datasets-2.1.0-py3-none-any.whl (325 kB)\n",
            "\u001b[K     |████████████████████████████████| 325 kB 30.7 MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from datasets) (1.21.6)\n",
            "Collecting fsspec[http]>=2021.05.0\n",
            "  Downloading fsspec-2022.3.0-py3-none-any.whl (136 kB)\n",
            "\u001b[K     |████████████████████████████████| 136 kB 67.8 MB/s \n",
            "\u001b[?25hRequirement already satisfied: tqdm>=4.62.1 in /usr/local/lib/python3.7/dist-packages (from datasets) (4.64.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from datasets) (21.3)\n",
            "Requirement already satisfied: dill in /usr/local/lib/python3.7/dist-packages (from datasets) (0.3.4)\n",
            "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.7/dist-packages (from datasets) (2.23.0)\n",
            "Collecting aiohttp\n",
            "  Downloading aiohttp-3.8.1-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (1.1 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.1 MB 58.2 MB/s \n",
            "\u001b[?25hRequirement already satisfied: pyarrow>=5.0.0 in /usr/local/lib/python3.7/dist-packages (from datasets) (6.0.1)\n",
            "Collecting xxhash\n",
            "  Downloading xxhash-3.0.0-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (212 kB)\n",
            "\u001b[K     |████████████████████████████████| 212 kB 72.1 MB/s \n",
            "\u001b[?25hRequirement already satisfied: multiprocess in /usr/local/lib/python3.7/dist-packages (from datasets) (0.70.12.2)\n",
            "Requirement already satisfied: huggingface-hub<1.0.0,>=0.1.0 in /usr/local/lib/python3.7/dist-packages (from datasets) (0.5.1)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from datasets) (4.11.3)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.7/dist-packages (from datasets) (1.3.5)\n",
            "Collecting responses<0.19\n",
            "  Downloading responses-0.18.0-py3-none-any.whl (38 kB)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from huggingface-hub<1.0.0,>=0.1.0->datasets) (3.6.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.7/dist-packages (from huggingface-hub<1.0.0,>=0.1.0->datasets) (4.1.1)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.7/dist-packages (from huggingface-hub<1.0.0,>=0.1.0->datasets) (6.0)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->datasets) (3.0.8)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->datasets) (2021.10.8)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->datasets) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->datasets) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->datasets) (1.24.3)\n",
            "Collecting urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1\n",
            "  Downloading urllib3-1.25.11-py2.py3-none-any.whl (127 kB)\n",
            "\u001b[K     |████████████████████████████████| 127 kB 75.8 MB/s \n",
            "\u001b[?25hCollecting frozenlist>=1.1.1\n",
            "  Downloading frozenlist-1.3.0-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (144 kB)\n",
            "\u001b[K     |████████████████████████████████| 144 kB 76.3 MB/s \n",
            "\u001b[?25hCollecting async-timeout<5.0,>=4.0.0a3\n",
            "  Downloading async_timeout-4.0.2-py3-none-any.whl (5.8 kB)\n",
            "Collecting asynctest==0.13.0\n",
            "  Downloading asynctest-0.13.0-py3-none-any.whl (26 kB)\n",
            "Collecting aiosignal>=1.1.2\n",
            "  Downloading aiosignal-1.2.0-py3-none-any.whl (8.2 kB)\n",
            "Collecting yarl<2.0,>=1.0\n",
            "  Downloading yarl-1.7.2-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (271 kB)\n",
            "\u001b[K     |████████████████████████████████| 271 kB 76.0 MB/s \n",
            "\u001b[?25hRequirement already satisfied: charset-normalizer<3.0,>=2.0 in /usr/local/lib/python3.7/dist-packages (from aiohttp->datasets) (2.0.12)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.7/dist-packages (from aiohttp->datasets) (21.4.0)\n",
            "Collecting multidict<7.0,>=4.5\n",
            "  Downloading multidict-6.0.2-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (94 kB)\n",
            "\u001b[K     |████████████████████████████████| 94 kB 4.1 MB/s \n",
            "\u001b[?25hRequirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->datasets) (3.8.0)\n",
            "Requirement already satisfied: pytz>=2017.3 in /usr/local/lib/python3.7/dist-packages (from pandas->datasets) (2022.1)\n",
            "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.7/dist-packages (from pandas->datasets) (2.8.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil>=2.7.3->pandas->datasets) (1.15.0)\n",
            "Installing collected packages: multidict, frozenlist, yarl, urllib3, asynctest, async-timeout, aiosignal, fsspec, aiohttp, xxhash, responses, datasets\n",
            "  Attempting uninstall: urllib3\n",
            "    Found existing installation: urllib3 1.24.3\n",
            "    Uninstalling urllib3-1.24.3:\n",
            "      Successfully uninstalled urllib3-1.24.3\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "datascience 0.10.6 requires folium==0.2.1, but you have folium 0.8.3 which is incompatible.\u001b[0m\n",
            "Successfully installed aiohttp-3.8.1 aiosignal-1.2.0 async-timeout-4.0.2 asynctest-0.13.0 datasets-2.1.0 frozenlist-1.3.0 fsspec-2022.3.0 multidict-6.0.2 responses-0.18.0 urllib3-1.25.11 xxhash-3.0.0 yarl-1.7.2\n"
          ]
        }
      ],
      "source": [
        "!pip install transformers\n",
        "!pip install datasets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mgfAS1Ew9lqM"
      },
      "outputs": [],
      "source": [
        "from transformers import AutoTokenizer, AutoModel, AutoModelForSequenceClassification\n",
        "from transformers import Trainer, TrainingArguments\n",
        "from transformers import DataCollatorWithPadding\n",
        "from datasets import load_dataset, load_metric\n",
        "import numpy as np\n",
        "\n",
        "from google.colab import drive\n",
        "import os"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wOoQNYwnB3B8"
      },
      "outputs": [],
      "source": [
        "# drive.mount('Drive')\n",
        "persistent_storage = 'Drive/My Drive/nlp-models/'\n",
        "persistent_storage = \"data\"\n",
        "os.makedirs(persistent_storage, exist_ok=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "995SEodD81kO"
      },
      "source": [
        "## Experiment 0: Understanding a pre-trained model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IOTXLbTiAfMe"
      },
      "source": [
        "We need two building blocks\n",
        "1. Tokenizer\n",
        "2. Model\n",
        "\n",
        "Pre-trained checkpoints can be found here: https://huggingface.co/models"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xfow53xW7-NO"
      },
      "outputs": [],
      "source": [
        "checkpoint = \"google/canine-s\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n",
        "model = AutoModel.from_pretrained(checkpoint)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "awDC29Aaf8zZ"
      },
      "outputs": [],
      "source": [
        "inputs = [\"Life is like a box of chocolates.\", \"Life they say\"]\n",
        "model_inputs = tokenizer(inputs, padding=\"longest\", truncation=True, return_tensors=\"pt\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XEgZVffhfp-4"
      },
      "source": [
        "- `return_tensors=\"pt\"`: the model returns PyTorch tensors\n",
        "\n",
        "- `padding=\"longest\"`: padds the shorter sequence with dummy tokens to match the longest one. The tensor `model_inputs.input_ids` has shape $(2, 35)$, where $2$ is the number of sentences and $35$ is the length of the longest sentence (the first). The second sentence is padded with the dummy $id=0$"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HJzDFe4S8r_s"
      },
      "outputs": [],
      "source": [
        "print(f\"Shape of encoded (tokenized) tensor: {model_inputs.input_ids.shape}\")\n",
        "\n",
        "# For CANINE, the id of the token is the Unicode number\n",
        "print(model_inputs)\n",
        "print(f\"Unicode ids: L={ord('L')}, i={ord('i')}, f={ord('f')}, L={ord('e')},\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MYAELIOPglxU"
      },
      "source": [
        "- Each token gets a feature (last_hidden_state) of shape $768$\n",
        "\n",
        "- The feature of the \"L\" character of the word \"Life\" in the first sentence is \n",
        "different to the feature of the \"L\" of the second sentence. The transformer takes into account the context (the rest of the sentence)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9tT19KG2Uqzu"
      },
      "outputs": [],
      "source": [
        "outputs = model(**model_inputs)\n",
        "\n",
        "print(outputs.last_hidden_state.shape)\n",
        "print(\"Features of L, first sentence: \" , outputs.last_hidden_state[0][1][:5])\n",
        "print(\"Features of L, second sentence: \" ,outputs.last_hidden_state[1][1][:5])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v5muWQMx85aI"
      },
      "source": [
        "## Fine-tuning a pre-trained model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Hr2XprlnAiZT"
      },
      "source": [
        "- We import the AutoModel for the desired fine-tunning task, for example `AutoModelForSequenceClassification`. This should be the pre-trained model with one extra layer specific to the task (the pre-trained model only outputs embeddings).\n",
        "\n",
        "- For Sequence Classification, we take two sequence as inputs and output the (logits) probability that both sentences are equivalent (1) or not equivalent (0).\n",
        "\n",
        "- For sentence entailment, since our original pre-trained model takes only one sequence as input, we'll have to do a wrapper to merge the two sentences and provide a way to distinguish them (this is done automatically).\n",
        "\n",
        "For running the experiments for different models you just need to select the appropiate checkpoint and run all the code."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Function encapsulating all the steps needed for fine-tuning and evaluating a model\n",
        "def train_and_evaluate(checkpoint, tokenizer, metrics_function, dataset, training_args, model_name, num_labels=None, eval_subset=\"validation\"):\n",
        "    # Load the model + tokenizer\n",
        "    tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n",
        "    if num_labels is None:\n",
        "        model = AutoModelForSequenceClassification.from_pretrained(checkpoint)\n",
        "    else:\n",
        "        model = AutoModelForSequenceClassification.from_pretrained(checkpoint, num_labels=num_labels)\n",
        "    data_collator = DataCollatorWithPadding(tokenizer=tokenizer)\n",
        "\n",
        "    trainer = Trainer(\n",
        "        model,\n",
        "        training_args,\n",
        "        train_dataset=dataset[\"train\"],\n",
        "        eval_dataset=dataset[eval_subset],\n",
        "        data_collator=data_collator,\n",
        "        tokenizer=tokenizer,\n",
        "        compute_metrics=metrics_function,\n",
        "    )\n",
        "\n",
        "    # Training\n",
        "    print(\"before training evaluation\")\n",
        "    trainer.evaluate()\n",
        "    print(\"start training\")\n",
        "    trainer.train()"
      ],
      "metadata": {
        "id": "o24WrEFCwjxA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TN_pIt2b6iTM"
      },
      "source": [
        "### Experiment 1: GLUE-MRPC"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Settings\n",
        "checkpoint = \"google/canine-c\"\n",
        "model_name = \"canine-c-glue-mrpc\"\n",
        "output_path = os.path.join(persistent_storage, model_name)\n",
        "tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n",
        "\n",
        "def tokenize_function(example):\n",
        "    \"\"\"\n",
        "    Returns new tokenizer function that merges the two sentence in example and \n",
        "    also provides a masking via the \"token_type_ids\" field. token_type_ids=0 \n",
        "    for the tokens of the sentence1 and token_type_ids=1 for those of sentence2\n",
        "    \"\"\"\n",
        "    return tokenizer(example[\"sentence1\"], \n",
        "                     example[\"sentence2\"], \n",
        "                     truncation=True,\n",
        "                     padding=True)\n",
        "\n",
        "def compute_metrics(eval_preds):\n",
        "    metric = load_metric(\"glue\", \"mrpc\")\n",
        "    logits, labels = eval_preds\n",
        "    predictions = np.argmax(logits, axis=-1)\n",
        "    return metric.compute(predictions=predictions, references=labels)\n",
        "\n",
        "    \n",
        "dataset = load_dataset(\"glue\", \"mrpc\").map(tokenize_function, batched=True)\n",
        "training_args = TrainingArguments(output_path,\n",
        "                                num_train_epochs=5,\n",
        "                                learning_rate=5e-5,\n",
        "                                per_device_train_batch_size=16,\n",
        "                                evaluation_strategy=\"epoch\",\n",
        "                                logging_steps=1,\n",
        "                                ) \n",
        "\n",
        "# Training + evaluation\n",
        "train_and_evaluate(checkpoint, tokenizer, compute_metrics, dataset, training_args, model_name)"
      ],
      "metadata": {
        "id": "j-i32EUNxuzY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Experiment 2: ajgt_twitter_ar"
      ],
      "metadata": {
        "id": "K58I2t8x2O_o"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Settings\n",
        "checkpoint = \"bert-base-multilingual-cased\"\n",
        "model_name = \"bert-cased-ajgt_twitter_ar\"\n",
        "output_path = os.path.join(persistent_storage, model_name)\n",
        "tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n",
        "\n",
        "def tokenize_function(example):\n",
        "    \"\"\"\n",
        "    Returns new tokenizer function that merges the two sentence in example and \n",
        "    also provides a masking via the \"token_type_ids\" field. token_type_ids=0 \n",
        "    for the tokens of the sentence1 and token_type_ids=1 for those of sentence2\n",
        "    \"\"\"\n",
        "    return tokenizer(example[\"text\"], \n",
        "                     truncation=True,\n",
        "                     padding=True)\n",
        "\n",
        "\n",
        "def compute_metrics(eval_preds):\n",
        "    metric = load_metric(\"glue\", \"mrpc\")\n",
        "    logits, labels = eval_preds\n",
        "    predictions = np.argmax(logits, axis=-1)\n",
        "    return metric.compute(predictions=predictions, references=labels)\n",
        "    \n",
        "\n",
        "raw_dataset = load_dataset(\"ajgt_twitter_ar\").map(tokenize_function, batched=True)\n",
        "dataset = raw_dataset[\"train\"].train_test_split()\n",
        "training_args = TrainingArguments(output_path,\n",
        "                                  num_train_epochs=5,\n",
        "                                  learning_rate=5e-5,\n",
        "                                  per_device_train_batch_size=16,\n",
        "                                  evaluation_strategy=\"epoch\",\n",
        "                                  logging_steps=1\n",
        "                                  ) \n",
        "\n",
        "# Training + evaluation\n",
        "train_and_evaluate(checkpoint, tokenizer, compute_metrics, dataset, training_args, model_name, eval_subset=\"test\")"
      ],
      "metadata": {
        "id": "T8E6rRX22ays"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0w1_sVih6sCN"
      },
      "source": [
        "### Experiment 3: fvillena/spanish_diagnostics "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fdctSTJS6vNh"
      },
      "outputs": [],
      "source": [
        "# Settings\n",
        "checkpoint = \"bert-base-multilingual-uncased\"\n",
        "model_name = \"bert-uncased-diagnostics\"\n",
        "output_path = os.path.join(persistent_storage, model_name)\n",
        "tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n",
        "\n",
        "def tokenize_function(example):\n",
        "    \"\"\"\n",
        "    Returns new tokenizer function that merges the two sentence in example and \n",
        "    also provides a masking via the \"token_type_ids\" field. token_type_ids=0 \n",
        "    for the tokens of the sentence1 and token_type_ids=1 for those of sentence2\n",
        "    \"\"\"\n",
        "    return tokenizer(example[\"text\"], \n",
        "                     truncation=True,\n",
        "                     padding=True)\n",
        "    \n",
        "    \n",
        "def compute_metrics(eval_preds):\n",
        "    metric = load_metric(\"glue\", \"mrpc\")\n",
        "    logits, labels = eval_preds\n",
        "    predictions = np.argmax(logits, axis=-1)\n",
        "    return metric.compute(predictions=predictions, references=labels)\n",
        "\n",
        "dataset = load_dataset(\"fvillena/spanish_diagnostics\").map(tokenize_function, batched=True)\n",
        "dataset[\"train\"] = dataset[\"train\"].select(list(range(3500)))\n",
        "dataset[\"test\"] = dataset[\"test\"].select(list(range(500)))\n",
        "training_args = TrainingArguments(output_path,\n",
        "                                  num_train_epochs=5,\n",
        "                                  learning_rate=5e-5,\n",
        "                                  per_device_train_batch_size=4,\n",
        "                                  evaluation_strategy=\"epoch\",\n",
        "                                  logging_steps=1,\n",
        "                                  ) \n",
        "\n",
        "# Training + evaluation\n",
        "train_and_evaluate(checkpoint, tokenizer, compute_metrics, dataset, training_args, model_name, eval_subset=\"test\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Experiment 4: amazon_reviews_multi (spanish split)\n"
      ],
      "metadata": {
        "id": "S7n4wfoScQCQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Settings\n",
        "checkpoint = \"google/canine-s\"\n",
        "model_name = \"canine-s-amazon\"\n",
        "output_path = os.path.join(persistent_storage, model_name)\n",
        "tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n",
        "\n",
        "def tokenize_function(example):\n",
        "    \"\"\"\n",
        "    Returns new tokenizer function that merges the two sentence in example and \n",
        "    also provides a masking via the \"token_type_ids\" field. token_type_ids=0 \n",
        "    for the tokens of the sentence1 and token_type_ids=1 for those of sentence2\n",
        "    \"\"\"\n",
        "    return tokenizer(example[\"review_body\"], \n",
        "                     truncation=True,\n",
        "                     padding=True)\n",
        "    \n",
        "    \n",
        "def compute_metrics(eval_preds):\n",
        "    f1 = load_metric(\"f1\")\n",
        "    accuracy = load_metric(\"accuracy\")\n",
        "    logits, labels = eval_preds\n",
        "    predictions = np.argmax(logits, axis=-1)\n",
        "    return {**f1.compute(predictions=predictions, references=labels, average=\"macro\"), **accuracy.compute(predictions=predictions, references=labels)}\n",
        "    \n",
        "dataset = load_dataset(\"amazon_reviews_multi\", \"es\").map(tokenize_function, batched=True)\n",
        "dataset = dataset.rename_column(\"stars\", \"label\")\n",
        "val_new_labels = list(map(lambda x: x-1, dataset['validation']['label']))\n",
        "test_new_labels = list(map(lambda x: x-1, dataset['test']['label']))\n",
        "dataset[\"validation\"] = dataset[\"validation\"].remove_columns('label')\n",
        "dataset[\"validation\"] = dataset[\"validation\"].add_column('label', val_new_labels)\n",
        "dataset[\"test\"] = dataset[\"test\"].remove_columns('label')\n",
        "dataset[\"test\"] = dataset[\"test\"].add_column('label', test_new_labels)\n",
        "# train set is huge, validation is already balanced!\n",
        "dataset[\"train\"] = dataset[\"validation\"]\n",
        "training_args = TrainingArguments(output_path,\n",
        "                                num_train_epochs=5,\n",
        "                                learning_rate=5e-5,\n",
        "                                per_device_train_batch_size=8,\n",
        "                                evaluation_strategy=\"epoch\",\n",
        "                                logging_steps=1,\n",
        "                                ) \n",
        "\n",
        "# Training + evaluation\n",
        "train_and_evaluate(checkpoint, tokenizer, compute_metrics, dataset, training_args, model_name, num_labels=5, eval_subset=\"test\")"
      ],
      "metadata": {
        "id": "vhIaCWlw7BTo"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [
        "cGOFiBSxte1h",
        "995SEodD81kO"
      ],
      "name": "MVA_Project_Algorithms_Speech_NLP.ipynb",
      "toc_visible": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}